{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"lmconf Documentation","text":"<p>Welcome to the lmconf documentation. This guide will help you understand how to use lmconf to configure and manage large language models in your applications.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To get started with lmconf, please refer to the Usage section.</p>"},{"location":"#api-reference","title":"API Reference","text":"<p>For a detailed description of the lmconf API, please see the API Reference section.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>If you're interested in contributing to lmconf, please see the Contributing section for guidelines and information.</p>"},{"location":"usage/","title":"lmconf Usage","text":"<p>This document provides a comprehensive guide on utilizing lmconf for configuring and managing large language models (LLMs) in your Python applications.</p>"},{"location":"usage/#installation","title":"Installation","text":"<p>Install lmconf using pip:</p> <pre><code>pip install lmconf\n</code></pre> <p>You can also install optional dependencies for specific LLM providers:</p> <ul> <li>LangChain integrations:</li> </ul> <p><code>bash   pip install lmconf[langchain]</code></p> <ul> <li>Tongyi (Alibaba Cloud):</li> </ul> <p><code>bash   pip install lmconf[tongyi]</code></p>"},{"location":"usage/#basic-configuration","title":"Basic Configuration","text":"<p>lmconf leverages environment variables and a structured configuration to define and manage LLM configurations. You can define your LLM configurations within a <code>.env</code> file or directly within your code.</p>"},{"location":"usage/#using-a-env-file","title":"Using a <code>.env</code> file","text":"<p>Create a <code>.env</code> file in your project root and configure your LLMs as follows:</p> <pre><code># Example .env file\nLMCONF_lm_config__config_list='[\n    {\"name\": \"azure_us\",\n     \"conf\": {\n        \"provider\": \"azure_openai\",\n        \"model\": \"gpt-35-turbo\",\n        \"api_version\": \"2023-05-15\",\n        \"base_url\": \"https://${COMPANY}-gpt.openai.azure.com\",\n        \"api_key\": \"${AZURE_US_API_KEY}\"\n     }},\n    {\"name\": \"zh_llm\",\n     \"conf\": {\n        \"provider\": \"tongyi\",\n        \"model\": \"qwen-max\",\n        \"api_key\": \"${DASHSCOPE_API_KEY}\"\n     }}\n]'\n\nLMCONF_lm_config__x='{\n    \"chatbot\": [\"azure_us\", \"gpt-35-turbo\"],\n    \"rag\": [\"zh_llm\", \"qwen-turbo\"]\n}'\n</code></pre> <p>Explanation:</p> <ul> <li><code>LMCONF_lm_config__config_list</code>: Defines a list of LLM configurations. Each configuration is a dictionary with:</li> <li>name: A unique identifier for the configuration.</li> <li>conf: A dictionary containing LLM-specific details like provider, model, API key, and base URL.</li> <li><code>LMCONF_lm_config__x</code>: Maps functionalities (like \"chatbot\" or \"rag\") to LLM configurations. This allows you to quickly select the appropriate LLM for a specific purpose.</li> </ul>"},{"location":"usage/#using-code","title":"Using code","text":"<p>You can also define your LLM configurations directly within your Python code:</p> <pre><code>from pydantic_settings import BaseSettings, SettingsConfigDict\nfrom lmconf import LMConfSettings\n\nclass Settings(BaseSettings, LMConfSettings):\n    # ... other settings ...\n\n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        env_prefix=\"LMCONF_\",\n        env_nested_delimiter=\"__\",\n    )\n\nsettings = Settings()\n</code></pre>"},{"location":"usage/#accessing-llms","title":"Accessing LLMs","text":"<p>Once you have defined your configurations, you can use lmconf to retrieve and use the LLMs:</p> <pre><code># Example using Settings class\nchatbot_llm = settings.lm_config.get(\"chatbot\").create_langchain_chatmodel(temperature=0.1)\noutput = chatbot_llm.invoke(\"What is the capital of France?\")\nprint(output)\n\n# Example using LMConfig directly\nlm_config = LMConfig(\n    config_list=[\n        {\n            \"name\": \"azure_us\",\n            \"conf\": {\n                \"provider\": \"azure_openai\",\n                \"model\": \"gpt-35-turbo\",\n                \"api_key\": \"your-api-key\",\n                \"base_url\": \"https://your-company-gpt.openai.azure.com\",\n                \"api_version\": \"2023-05-15\",\n            }\n        }\n    ],\n    x={\n        \"chatbot\": [\"azure_us\", \"gpt-35-turbo\"],\n    }\n)\n\nchatbot_llm = lm_config.get(\"chatbot\").create_langchain_chatmodel(temperature=0.1)\noutput = chatbot_llm.invoke(\"What is the capital of France?\")\nprint(output)\n</code></pre>"},{"location":"usage/#example-using-lmconf-with-langchain","title":"Example: Using lmconf with LangChain","text":"<p>This example demonstrates how to use lmconf with LangChain for creating a simple chatbot:</p> <pre><code>from lmconf import LMConfSettings\nfrom langchain_community.chat_message_histories import SQLChatMessageHistory\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\n\nclass Settings(BaseSettings, LMConfSettings):\n    SQLALCHEMY_DATABASE_URI: str\n\n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        env_prefix=\"lc_chatbot_\",\n        env_nested_delimiter=\"__\",\n    )\n\nsettings = Settings()\n\nclass Chatbot:\n    def __init__(self, session_id: str):\n        self.session_id = session_id\n\n        llm = settings.lm_config.get(\"chatbot\").create_langchain_chatmodel(temperature=0.1)\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\"system\", \"You are a helpful assistant.\"),\n                MessagesPlaceholder(variable_name=\"history\"),\n                (\"human\", \"{question}\"),\n            ]\n        )\n        chain = prompt | llm\n\n        self.chatbot = RunnableWithMessageHistory(\n            chain,\n            lambda session_id: SQLChatMessageHistory(session_id=session_id, connection=settings.SQLALCHEMY_DATABASE_URI),\n            input_messages_key=\"question\",\n            history_messages_key=\"history\",\n        )\n\n    def __call__(self, question):\n        message = self.chatbot.invoke({\"question\": question}, config={\"configurable\": {\"session_id\": self.session_id}})\n        return message\n\nif __name__ == \"__main__\":\n    import uuid\n    chatbot = Chatbot(uuid.uuid4().hex)\n\n    message = chatbot(\"Hello! I'm Bob.\")\n    print(message)\n\n    message = chatbot(\"What's my name?\")\n    print(message)\n</code></pre>"},{"location":"usage/#advanced-features","title":"Advanced Features","text":""},{"location":"usage/#custom-llm-configurations","title":"Custom LLM Configurations","text":"<p>You can create custom LLM configuration classes by extending <code>LLMConfBase</code> from lmconf. This allows you to define provider-specific parameters and implement custom logic for creating LangChain models.</p>"},{"location":"usage/#environmental-variables","title":"Environmental Variables","text":"<p>lmconf utilizes environment variables for flexible configuration. You can set environment variables in your <code>.env</code> file or directly using shell commands.</p>"},{"location":"usage/#caching","title":"Caching","text":"<p><code>EnvCacheSettingsMixin</code> uses a caching mechanism to avoid unnecessary validation and improve performance when accessing LLM configurations from environment variables.</p>"},{"location":"usage/#conclusion","title":"Conclusion","text":"<p>lmconf simplifies the configuration and management of LLMs in your Python applications, providing a robust and flexible framework for integrating LLMs into your projects.</p>"}]}